ggtitle('MPG vs HP Poly') +
xlab('Horsepower') +
ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +
geom_line(aes(x = mtcars$hp, y = predict(my_poly_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Poly') +
xlab('Horsepower') +
ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +
geom_line(aes(x = mtcars$hp, y = predict(my_poly_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Poly') +
xlab('Horsepower') +
ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +
geom_line(aes(x = mtcars$hp, y = predict(my_poly_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Poly') +
xlab('Horsepower') +
ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +
geom_line(aes(x = mtcars$hp, y = predict(my_poly_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Poly') +
xlab('Horsepower') +
ylab('MPG')
data(mtcars)
my_linear_model = lm(formula = mpg ~ hp, data = mtcars)
summary(my_linear_model)
mtcars$hp2 = mtcars$hp^2
my_poly_model = lm(formula = mpg ~ hp + hp2, data = mtcars)
summary(my_poly_model)
library(ggplot2)
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red')
+geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +
geom_line(aes(x = mtcars$hp, y = predict(my_poly_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Poly') +xlab('Horsepower') + ylab('MPG')
library(ggplot2)
ggplot() +geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red')
+geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
library(ggplot2)
ggplot() +
geom_point(aes(x = mtcars$hp, y = mtcars$mpg), colour = 'red') +geom_line(aes(x = mtcars$hp, y = predict(my_linear_model, newdata = mtcars)), colour = 'blue') +
ggtitle('MPG vs HP Linear') +xlab('Horsepower') +ylab('MPG')
# Load dataset
dataset = read.csv("C:/Local volume/Programming/Machine Learning/Datasets/Position_Salaries.csv")
dataset
# Install required package
#install.packages("gmodels")
# Add higher degree levels to the dataset (quadratic, cubic, quartic)
dataset$Level2 = dataset$Level^2
dataset$Level3 = dataset$Level^3
dataset$Level4 = dataset$Level^4
# Create the linear model with all 4 levels (Level, Level^2, Level^3, Level^4)
my_model <- lm(Salary ~ Level + Level2 + Level3 + Level4, data=dataset)
summary(my_model)
# Visualize the results
library(ggplot2)
ggplot() +
geom_point(aes(x = dataset$Level, y = dataset$Salary), colour = 'red') +
geom_line(aes(x = dataset$Level, y = predict(my_model, newdata = dataset)), colour = 'blue') +
ggtitle('Truth or Bluff (Polynomial Regression)') +
xlab('Level') +
ylab('Salary')
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree)
cv_tree = rpart(target, data = carseats_train, method = "class", control = rpart.control(cp = 0.01))
printcp(cv_tree)
pruned_tree = prune(cv_tree, cp = cv_tree$cptable[9, "CP"])
predictions = predict(pruned_tree, carseats_test, type = "class")
confusion_matrix = table(Predicted = predictions, Actual = carseats_test$High)
confusion_matrix
library(ggplot2)
ggplot(data = as.data.frame(confusion_matrix), aes(x = Actual, y = Predicted)) +
geom_tile(aes(fill = Freq), color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(title = "Confusion Matrix", x = "Actual High Sales", y = "Predicted High Sales")
install.packages("ISLR")
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales >= 8, "Yes", "No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
library(rpart)
target = High ~ . - Sales
tree = rpart(target, data = carseats_train, method = "class")
install.packages("rpart.plot")
install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree)
cv_tree = rpart(target, data = carseats_train, method = "class", control = rpart.control(cp = 0.01))
printcp(cv_tree)
pruned_tree = prune(cv_tree, cp = cv_tree$cptable[9, "CP"])
predictions = predict(pruned_tree, carseats_test, type = "class")
confusion_matrix = table(Predicted = predictions, Actual = carseats_test$High)
confusion_matrix
library(ggplot2)
ggplot(data = as.data.frame(confusion_matrix), aes(x = Actual, y = Predicted)) +
geom_tile(aes(fill = Freq), color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(title = "Confusion Matrix", x = "Actual High Sales", y = "Predicted High Sales")
install.packages("gmodels")
library(gmodels)
confusion_matrix = table(Predicted = predictions, Actual = carseats_test$High)
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales >= 8, "Yes", "No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
library(rpart)
target = High ~ . - Sales
tree = rpart(target, data = carseats_train, method = "class")
#install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree)
cv_tree = rpart(target, data = carseats_train, method = "class", control = rpart.control(cp = 0.01))
printcp(cv_tree)
pruned_tree = prune(cv_tree, cp = cv_tree$cptable[9, "CP"])
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales >= 8, "Yes", "No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
library(rpart)
target = High ~ . - Sales
tree = rpart(target, data = carseats_train, method = "class")
library(rpart.plot)
rpart.plot(tree)
predictions = predict(tree, carseats_test, type = "class")
library(gmodels)
confusion_matrix = table(Predicted = predictions, Actual = carseats_test$High)
confusion_matrix
library(ggplot2)
ggplot(data = as.data.frame(confusion_matrix), aes(x = Actual, y = Predicted)) +
geom_tile(aes(fill = Freq), color = "white") +
scale_fill_gradient(low = "white", high = "blue") +
theme_minimal() +
labs(title = "Confusion Matrix", x = "Actual High Sales", y = "Predicted High Sales")
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales >= 8, "Yes", "No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
library(rpart)
target = High ~ . - Sales
tree = rpart(target, data = carseats_train, method = "class")
library(rpart.plot)
rpart.plot(tree)
predictions = predict(tree, carseats_test, type = "class")
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales >= 8, "Yes", "No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
library(rpart)
target = High ~ . - Sales
tree = rpart(target, data = carseats_train, method = "class")
library(rpart.plot)
rpart.plot(tree)
predictions = predict(tree, carseats_test, type = "class")
library(ISLR)
data(Carseats)
dataset = Carseats
dataset$High = ifelse(dataset$Sales>=8,"Yes","No")
dataset$High = as.factor(dataset$High)
indexes = sample(1:nrow(dataset), nrow(dataset) * 0.7)
carseats_train = dataset[indexes, ]
carseats_test = dataset[-indexes, ]
#install.packages("rpart")
library(rpart)
target = High ~ . -Sales
tree = rpart(target, data=carseats_train, method="class")
#install.packages("rpart.plot")
library(rpart.plot)
rpart.plot(tree)
predictions = predict(tree,  carseats_test,type="class")
data<- read.csv("C:\Local volume\Programming\Machine Learning\CA\CA2")
data
data<- read.csv("C:\Local volume\Programming\Machine Learning\CA\CA2\heart.csv")
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_2 <-as.data.frame(lapply(data[,1:13], normalize))
target
data_2
str(data)
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
data_train<-data_2[1:49]
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_2 <-as.data.frame(lapply(data[,1:13], normalize))
data_2
data_train<-data_2[1:49]
data_2
my_varibales <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_2 <-as.data.frame(lapply(data[,1:13], normalize))
data_2
my_varibales <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data[my_varibales] <- lapply(data[my_varibales], normalize)
train_index <- data$target, p = 0.7, list = FALSE)
data_train<-data_2 <- data$target, p = 0.7, list = FALSE)
train_data <- heart_data[train_index, ]
test_data <- heart_data[-train_index, ]
install.packages("carat")
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data[my_variables] <- lapply(data[my_variables], normalize)
library(carat)
train_index <- createDataPartition(data$target, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
?createDataPartition
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_2 <-as.data.frame(lapply(data[,1:13], normalize))
data_2
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data[my_variables] <- lapply(data[my_variables], normalize)
install.packages("carat")
library(carat)
train_index <- createDataPartition(data$target, p = 0.7, list = FALSE)
train_data <- data[train_index, ]
test_data <- data[-train_index, ]
install.packages("class")
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data[my_variables] <- lapply(data[my_variables], normalize)
test_data<-data_2[1:49]
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data_2 <-as.data.frame(lapply(data[,1:13], normalize))
data_2
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
data[my_variables] <- lapply(data[my_variables], normalize)
test_data<-data_2[1:49]
data<- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data
str(data)
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data[, 1:13] <- lapply(data[, 1:13], normalize)
train_index <- sample(1:nrow(data), 0.7 * nrow(data)) # 70% for training
data_train <- data[train_index, ]
data_test <- data[-train_index, ]
train_index <- sample(1:nrow(data), 0.7 * nrow(data))
data_train <- data[train_index, ]
data_test <- data[-train_index, ]
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
install.packages("class")
library("class")
k <- 21
data_predicted_label <- knn(data_train[, my_variables],
data_test[, my_variables],
cl = data_train$target,
k = k)
# Evaluate the model
confusionMatrix(data_predicted_label, data_test$target)
install.packages("gmodels")
library("gmodels")
confusionMatrix(data_predicted_label, data_test$target)
confusionMatrix(data_predicted_label, data_test$target)
library("gmodels")
confusionMatrix(data_predicted_label, data_test$target)
# Install and load the caret package
install.packages("caret")
confusionMatrix(data_predicted_label, data_test$target)
install.packages("gmodels")
library(gmodels)
confusionMatrix(data_predicted_label, data_test$target)
install.packages("gmodels")
confusionMatrix(data_predicted_label, data_test$target)
CrossTable(x = data_test, y = data_test_predict, prop.chisq = FALSE)
aa <- table(data_test_labels, data_test_predict)
CrossTable(aa)
data <- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
data$target <- as.factor(data$target)
normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x)))
}
data[, 1:13] <- lapply(data[, 1:13], normalize)
train_index <- sample(1:nrow(data), 0.7 * nrow(data))
data_train <- data[train_index, ]
data_test <- data[-train_index, ]
my_variables <- c("age", "trestbps", "chol", "thalach", "oldpeak")
install.packages("class")
k <- 21
data_predict_label <- knn(data_train[, my_variables],
data_test[, my_variables],
cl = data_train$target,
k = k)
install.packages("gmodels")
# Use data_predict_label, not data_test_predict
confusionMatrix(data_predict_label, data_test$target)
library(e1071)
library(caret)
library(gmodels)
heart_data <- read.csv("heart.csv")
heart_data <- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
heart_data$sex <- as.factor(heart_data$sex)
heart_data$cp <- as.factor(heart_data$cp)
heart_data$fbs <- as.factor(heart_data$fbs)
heart_data$restecg <- as.factor(heart_data$restecg)
heart_data$exang <- as.factor(heart_data$exang)
heart_data$slope <- as.factor(heart_data$slope)
heart_data$thal <- as.factor(heart_data$thal)
heart_data$target <- as.factor(heart_data$target)
split_index <- createDataPartition(heart_data$target, p = 0.8, list = FALSE)
data_train <- heart_data[split_index, ]
data_test <- heart_data[-split_index, ]
nb_model <- naiveBayes(target ~ ., data = data_train)
nb_predictions <- predict(nb_model, newdata = data_test)
confusionMatrix(nb_predictions, data_test$target)
CrossTable(nb_predictions, data_test$target, prop.chisq = FALSE)
library(e1071)
library(caret)
library(gmodels)
heart_data <- read.csv("C:/Local volume/Programming/Machine Learning/CA/CA2/heart.csv")
heart_data$sex <- as.factor(heart_data$sex)
heart_data$cp <- as.factor(heart_data$cp)
heart_data$fbs <- as.factor(heart_data$fbs)
heart_data$restecg <- as.factor(heart_data$restecg)
heart_data$exang <- as.factor(heart_data$exang)
heart_data$slope <- as.factor(heart_data$slope)
heart_data$thal <- as.factor(heart_data$thal)
heart_data$target <- as.factor(heart_data$target)
split_index <- createDataPartition(heart_data$target, p = 0.8, list = FALSE)
data_train <- heart_data[split_index, ]
data_test <- heart_data[-split_index, ]
nb_model <- naiveBayes(target ~ ., data = data_train)
nb_predictions <- predict(nb_model, newdata = data_test)
confusionMatrix(nb_predictions, data_test$target)
CrossTable(nb_predictions, data_test$target, prop.chisq = FALSE)
R.home()
install.packages("renv")  # Install renv if it's not installed
renv::activate()  # Activate the renv environment
install.packages("tm")
install.packages("SnowballC")
library(tm)
library(SnowballC)
setwd("C:/Local volume/Programming/Machine Learning/Dashboard")
# Load necessary libraries
library(readr)
library(dplyr)
install.packages("readr")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("caret")
install.packages("caret")
install.packages("class")
install.packages("e1071")
install.packages("rpart")
install.packages("rpart.plot")
library(readr)
library(dplyr)
library(ggplot2)
library(caret)
library(class)
library(e1071)
library(rpart)
library(rpart.plot)
# Load the dataset
heart_data <- read_csv("C:/Local volume/Programming/Machine Learning/Dashboard/heart.csv")
str(heart_data)
summary(heart_data)
head(heart_data, 5)
# Basic visualization of data distribution
ggplot(heart_data, aes(x = age)) + geom_histogram(bins = 10, fill = "blue", color = "black") +
ggtitle("Distribution of Age")
# Correlation matrix visualization
correlations <- cor(heart_data[, sapply(heart_data, is.numeric)])
corrplot(correlations, method = "circle")
# Correlation matrix visualization
correlations <- cor(heart_data[, sapply(heart_data, is.numeric)])
corrplot(correlations, method = "circle")
install.packages("corrplot")
library(corrplot)
# Correlation matrix visualization
correlations <- cor(heart_data[, sapply(heart_data, is.numeric)])
corrplot(correlations, method = "circle")
# Check for missing values
sum(is.na(heart_data))
# Impute missing values with the median
heart_data <- heart_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Check for missing values
sum(is.na(heart_data))
heart_data <- heart_data %>%
mutate(across(where(is.numeric), ~ ifelse(is.na(.), median(., na.rm = TRUE), .)))
# Load the dataset
heart_data <- read_csv("C:/Local volume/Programming/Machine Learning/Dashboard/heart.csv")
# Initial exploration
str(heart_data)
summary(heart_data)
library(readr)
library(corrplot)
library(dplyr)
library(ggplot2)
library(caret)
library(class)
library(e1071)
library(rpart)
library(rpart.plot)
heart_data <- read_csv("C:/Local volume/Programming/Machine Learning/Dashboard/heart.csv")
# Initial exploration
str(heart_data)
summary(heart_data)
head(heart_data, 5)
ggplot(heart_data, aes(x = age)) + geom_histogram(bins = 10, fill = "blue", color = "black") +
ggtitle("Distribution of Age")
# Correlation matrix visualization
correlations <- cor(heart_data[, sapply(heart_data, is.numeric)])
corrplot(correlations, method = "circle")
